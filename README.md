# Задача: по последовательности из нескольких веб-сайтов, посещенных подряд одним и тем же человеком, надо идентифицировать этого человека. Идея такая: пользователи Интернета по-разному переходят по ссылкам, и это может помогать их идентифицировать (кто-то сначала в почту, потом про футбол почитать, затем новости, контакт, потом наконец – работать, кто-то – сразу работать, если это возможно).

Данные берутся из статьи "A Tool for Classification of Sequential Data". 

Имеются данные с прокси-серверов Университета Блеза Паскаля, их вид очень простой: ID пользователя, timestamp, посещенный веб-сайт.

Скачать исходные данные можно по ссылке в статье (там же описание), для этого задания хватит данных не по всем 3000 пользователям, а по 10 и 150. Ссылка на архив capstone_user_identification.zip (~7 Mb, в развернутом виде ~60 Mb).

В этом проекте решается задача идентификации пользователя по его поведению в сети Интернет. Это сложная и интересная задача на стыке анализа данных и поведенческой психологии. В качестве примера, компания Яндекс решает задачу идентификации взломщика почтового ящика по его поведению. В двух словах, взломщик будет себя вести не так, как владелец ящика: он может не удалять сообщения сразу по прочтении, как это делал хозяин, он будет по-другому ставить флажки сообщениям и даже по-своему двигать мышкой. Тогда такого злоумышленника можно идентифицировать и "выкинуть" из почтового ящика, предложив хозяину войти по SMS-коду.

Решается следующая задача: по последовательности из нескольких веб-сайтов, посещенных подряд один и тем же человеком, будем идентифицировать этого человека. Идея такая: пользователи Интернета по-разному переходят по ссылкам, и это может помогать их идентифицировать (кто-то сначала в почту, потом про футбол почитать, затем новости, контакт, потом наконец – работать, кто-то – сразу работать).

План проекта такой:
- Первая часть проекта посвящена подготовке данных для дальнейшего описательного анализа и построения прогнозных моделей. Предобработка данных (исходно посещенные веб-сайты указаны для каждого пользователя в отдельном файле) и формирование единой обучающей выборки. Также в этой части воспользуемся разреженным форматом данных (матрицы Scipy.sparse), который хорошо подходит для данной задачи.

- Подготовка данных для дальнейшего анализа и построения прогнозных моделей. Определение оптимальной длины сессии параметром при обучении на прогнозных моделях. Проверка статистических гипотез.

- Визуальным анализ данных и построение признаков.

- Сравнение алгоритмов классификации. Сравнение на кросс-валидации несколько алгоритмов, подбор паарметров длины сессии (session_length и window_size). Построение кривых валидации (как качество классификации зависит от одного из гиперпараметров алгоритма) и кривых обучения (как качество классификации зависит от объема выборки).
